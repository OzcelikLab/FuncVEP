{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on new variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def impute_missing_values(df):\n",
    "    imputer_dir = \"../models/imputation\"\n",
    "\n",
    "    with open(\"../resources/feature_lists/columns_to_impute.txt\", \"r\") as f:\n",
    "        columns_to_impute = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "    id_column = df[\"ID\"].copy()\n",
    "\n",
    "    df = df.apply(lambda col: pd.to_numeric(col, errors='coerce') if col.name != \"ID\" else col)\n",
    "\n",
    "    df[\"ID\"] = id_column\n",
    "\n",
    "    for col in columns_to_impute:\n",
    "        model_path = os.path.join(imputer_dir, f\"{col}_imputer.pkl\")\n",
    "\n",
    "        if col not in df.columns:\n",
    "            print(f\"Skipping {col}: not found in dataframe.\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Skipping {col}: imputation model not found.\")\n",
    "            continue\n",
    "\n",
    "        missing_mask = df[col].isna()\n",
    "        if missing_mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        imputer = joblib.load(model_path)\n",
    "        predictors = imputer.feature_name_\n",
    "\n",
    "        # Subset the DataFrame to rows with missing target values and the required predictors\n",
    "        available_predictors = [p for p in predictors if p in df.columns]\n",
    "        X_pred = df.loc[missing_mask, available_predictors].copy()\n",
    "\n",
    "        # Reorder columns to match the model's expected input\n",
    "        X_pred = X_pred.reindex(columns=predictors)\n",
    "\n",
    "        if not X_pred.empty:\n",
    "            df.loc[missing_mask, col] = imputer.predict(X_pred)\n",
    "            print(f\"Imputed {missing_mask.sum()} missing values in {col}.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_name, df, dataset):\n",
    "    model_dir = f\"../models/{model_name}\"\n",
    "    output_path = f\"../results/predictions/inference/{dataset}/{model_name}.txt\"\n",
    "\n",
    "    trained_on = pd.read_csv(os.path.join(model_dir, \"training_variants.txt\"), sep=\"\\t\")\n",
    "    df = df[~df[\"ID\"].isin(trained_on[\"ID\"])].copy()\n",
    "\n",
    "    lgb_model = joblib.load(os.path.join(model_dir, \"model.pkl\"))\n",
    "    trained_features = lgb_model.feature_name_\n",
    "\n",
    "    df[trained_features] = df[trained_features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    X = df[trained_features]\n",
    "    df[model_name] = lgb_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    result_df = df[[\"ID\", model_name]]\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    result_df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkere\\AppData\\Local\\Temp\\ipykernel_16364\\3476502212.py:2: DtypeWarning: Columns (31,32,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_input = pd.read_csv(\"../data/inference/IEI_variants_model_input.txt\", sep=\"\\t\").drop_duplicates(subset=[\"ID\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 18765 missing values in MetaLR_score.\n",
      "Imputed 20015 missing values in MutScore_score.\n",
      "Imputed 16688 missing values in fathmm_XF_coding_score.\n",
      "Imputed 21818 missing values in MutFormer_score.\n",
      "Imputed 18765 missing values in MetaSVM_score.\n",
      "Imputed 41505 missing values in VEST4_score.\n",
      "Imputed 159607 missing values in EVH_epistatic.\n",
      "Imputed 159607 missing values in EVH_independent.\n",
      "Imputed 30136 missing values in M_CAP_score.\n",
      "Imputed 45483 missing values in DEOGEN2_score.\n",
      "Imputed 139401 missing values in glm_CaddDeogenRevel.\n",
      "Imputed 63247 missing values in glm_AlphDeogenRevel.\n",
      "Imputed 63132 missing values in glm_AlphCaddDeogen.\n",
      "Imputed 62812 missing values in glm_AlphRevelCadd.\n",
      "Imputed 62499 missing values in glm_AlphRevel.\n",
      "Imputed 43838 missing values in glm_DeogenRevel.\n",
      "Imputed 42308 missing values in glm_RevelCadd.\n",
      "Imputed 64109 missing values in MutPred_score.\n",
      "Imputed 63360 missing values in REVEL_score.\n",
      "Imputed 45779 missing values in MetaRNN_score.\n",
      "Imputed 52184 missing values in VARITY_ER_LOO_score.\n",
      "Imputed 52184 missing values in VARITY_ER_score.\n",
      "Imputed 52184 missing values in VARITY_R_LOO_score.\n",
      "Imputed 52184 missing values in VARITY_R_score.\n",
      "Imputed 50837 missing values in glm_AlphDeogen.\n",
      "Imputed 29378 missing values in glm_CaddDeogen.\n",
      "Imputed 18361 missing values in ClinPred_score.\n",
      "Imputed 33550 missing values in Polyphen2_HVAR_score.\n",
      "Imputed 92362 missing values in MPC_score.\n",
      "Imputed 112282 missing values in sigma_score.\n",
      "Imputed 68319 missing values in LIST_S2_score.\n",
      "Imputed 48989 missing values in glm_AlphCadd.\n",
      "Imputed 24104 missing values in AlphScore.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkere\\AppData\\Local\\Temp\\ipykernel_16364\\3863842653.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[model_name] = lgb_model.predict_proba(X)[:, 1]\n",
      "C:\\Users\\hkere\\AppData\\Local\\Temp\\ipykernel_16364\\3863842653.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[model_name] = lgb_model.predict_proba(X)[:, 1]\n",
      "C:\\Users\\hkere\\AppData\\Local\\Temp\\ipykernel_16364\\3863842653.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[model_name] = lgb_model.predict_proba(X)[:, 1]\n",
      "C:\\Users\\hkere\\AppData\\Local\\Temp\\ipykernel_16364\\3863842653.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[model_name] = lgb_model.predict_proba(X)[:, 1]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"IEI_variants\"\n",
    "df_input = pd.read_csv(\"../data/inference/IEI_variants_model_input.txt\", sep=\"\\t\").drop_duplicates(subset=[\"ID\"])\n",
    "\n",
    "df_imputed = impute_missing_values(df_input.copy())\n",
    "\n",
    "funcvep_cti = run_inference(\"FuncVEP_CTI\", df_imputed, dataset)\n",
    "funcvep_cte = run_inference(\"FuncVEP_CTE\", df_imputed, dataset)\n",
    "funcvep_sp = run_inference(\"FuncVEP_SP\", df_imputed, dataset)\n",
    "clinvep = run_inference(\"ClinVEP\", df_imputed, dataset)\n",
    "\n",
    "models_combined = funcvep_cti.merge(funcvep_cte, on=\"ID\", how=\"outer\").merge(funcvep_sp, on=\"ID\", how=\"outer\").merge(clinvep, on=\"ID\", how=\"outer\")\n",
    "\n",
    "models_combined.to_csv(f\"../results/predictions/inference/{dataset}/models_combined.txt\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
