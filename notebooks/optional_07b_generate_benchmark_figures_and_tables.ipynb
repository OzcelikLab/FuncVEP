{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de259d6",
   "metadata": {},
   "source": [
    "## Generate benchmarking figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b781f",
   "metadata": {},
   "source": [
    "#### Generate legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aba7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib.patches import Patch\n",
    "from pathlib import Path\n",
    "\n",
    "font_path = \"../resources/fonts/Aptos.ttf\"\n",
    "fm.fontManager.addfont(font_path)\n",
    "prop = fm.FontProperties(fname=font_path)\n",
    "mpl.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "def save_legend(boxed: bool = True, show_title: bool = True):\n",
    "\n",
    "    color_map = {\n",
    "        \"Functional-trained\": \"#000080\",\n",
    "        \"Clinical-trained (meta)\": \"#b21e35\",\n",
    "        \"Clinical-trained (single)\": \"goldenrod\",\n",
    "        \"Population-free\": \"#98e5a5\",\n",
    "        \"Population-tuned\": \"#23a4a6\",\n",
    "    }\n",
    "\n",
    "    def _make_handles(linewidth=0.4):\n",
    "        return [\n",
    "            Patch(facecolor=color_map[cat], edgecolor='black', linewidth=linewidth, label=cat)\n",
    "            for cat in color_map\n",
    "        ]\n",
    "\n",
    "    def _save(ax, out_path):\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.tight_layout()\n",
    "        fig = ax.figure\n",
    "        fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved legend to: {out_path}\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2.2, 1))\n",
    "    ax.axis(\"off\")\n",
    "    handles = _make_handles(linewidth=0.4)\n",
    "    legend = ax.legend(\n",
    "        handles=handles,\n",
    "        loc=\"center\",\n",
    "        fontsize=8,\n",
    "        title=\"Category\" if show_title else None,\n",
    "        title_fontsize=9,\n",
    "        ncol=1,\n",
    "        frameon=boxed,\n",
    "        edgecolor=\"black\" if boxed else None,\n",
    "        labelspacing=0.5,\n",
    "        borderpad=0.6,\n",
    "        handlelength=1.2,\n",
    "        handletextpad=0.6,\n",
    "        borderaxespad=0.0\n",
    "    )\n",
    "    if show_title:\n",
    "        legend.get_title().set_position((0, 4))\n",
    "    _save(ax, Path(\"../results/figures/benchmarks/legend_vertical_box.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a2beb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved legend to: ..\\results\\figures\\benchmarks\\legend_vertical_box.png\n"
     ]
    }
   ],
   "source": [
    "save_legend(boxed=False, show_title=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4159445",
   "metadata": {},
   "source": [
    "#### Generate AUC bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import font_manager as fm\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "\n",
    "font_path = \"../resources/fonts/Aptos.ttf\"\n",
    "fm.fontManager.addfont(font_path)\n",
    "prop = fm.FontProperties(fname=font_path)\n",
    "mpl.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "def generate_benchmark_figure(\n",
    "    dataset_name=\"functional\",\n",
    "    include_legend=True,\n",
    "    remove_clinvep=True,\n",
    "    len_x=None,\n",
    "    filter_glm=False,\n",
    "    linewidth=0.5,\n",
    "    gap_frac=0.25,\n",
    "    outer_pad_frac=1,\n",
    "    bar_height=0.3\n",
    "):\n",
    "\n",
    "    if dataset_name.startswith(\"mave_\"):\n",
    "        score_col = \"Mean_R2\"\n",
    "        correlation_type = dataset_name.replace(\"mave_\", \"\")\n",
    "        data_file = f\"../results/benchmarking/mave_{correlation_type}_correlation.txt\"\n",
    "        x_label = \"Mean R²\"\n",
    "        plot_title = \"Mean R² across MAVE studies\"\n",
    "        default_len_x = 0.35\n",
    "    else:\n",
    "        score_col = \"AUC\"\n",
    "        data_file = f\"../results/benchmarking/{dataset_name}.txt\"\n",
    "        x_label = \"AUC\"\n",
    "        default_len_x = 1.0\n",
    "        if dataset_name == \"cancer\":\n",
    "            plot_title = \"Performance on cancer hotspots\"\n",
    "        elif dataset_name in [\"clinical\", \"functional\"]:\n",
    "            plot_title = f\"Performance on {'clinical' if dataset_name == 'clinical' else 'functional'} data\"\n",
    "        else:\n",
    "            plot_title = f\"Performance on {dataset_name.upper()} variants\"\n",
    "\n",
    "    len_x = len_x if len_x is not None else default_len_x\n",
    "    features_file = \"../resources/feature_lists/all_columns.txt\"\n",
    "    output_figure_path = f\"../results/figures/benchmarks/{dataset_name}_auc_plot.png\"\n",
    "\n",
    "    df = pd.read_csv(data_file, sep=\"\\t\")\n",
    "    features_df = pd.read_csv(features_file, sep=\"\\t\").rename(columns={\"Name\": \"Tool\"})\n",
    "\n",
    "    vep_tools = features_df[features_df[\"Category\"] == \"Variant Effect Predictor\"][\"Tool\"].unique()\n",
    "    df = df[~df[\"Tool\"].isin(vep_tools)].copy()\n",
    "\n",
    "    if filter_glm:\n",
    "        if dataset_name in [\"clinical\", \"functional\"]:\n",
    "            other = \"functional\" if dataset_name == \"clinical\" else \"clinical\"\n",
    "            other_df = pd.read_csv(f\"../results/benchmarking/{other}.txt\", sep=\"\\t\")\n",
    "            best_glm_current = df[df[\"Tool\"].str.startswith(\"glm_\")].nlargest(1, score_col)\n",
    "            best_glm_other = other_df[other_df[\"Tool\"].str.startswith(\"glm_\")].nlargest(1, score_col)\n",
    "            glm_to_keep = pd.concat([best_glm_current, best_glm_other])[\"Tool\"].unique()\n",
    "            df = df[(~df[\"Tool\"].str.startswith(\"glm_\")) | (df[\"Tool\"].isin(glm_to_keep))].copy()\n",
    "        else:\n",
    "            best_glm = df[df[\"Tool\"].str.startswith(\"glm_\")].nlargest(1, score_col)\n",
    "            glm_to_keep = best_glm[\"Tool\"].unique()\n",
    "            df = df[(~df[\"Tool\"].str.startswith(\"glm_\")) | (df[\"Tool\"].isin(glm_to_keep))].copy()\n",
    "\n",
    "    df = df.merge(features_df[[\"Tool\", \"Category\"]], on=\"Tool\", how=\"left\")\n",
    "\n",
    "    if remove_clinvep:\n",
    "        df = df[~df[\"Tool\"].str.startswith(\"ClinVEP_\")].copy()\n",
    "\n",
    "    def clean_tool_name(tool):\n",
    "        tool = re.sub(r\"^glm_\", \"\", tool)\n",
    "        tool = re.sub(r\"_score$\", \"\", tool)\n",
    "        return tool.replace(\"___\", \"-\").replace(\"__\", \"-\").replace(\"_\", \"-\")\n",
    "\n",
    "    df[\"Tool\"] = df[\"Tool\"].apply(clean_tool_name)\n",
    "\n",
    "    category_rename = {\n",
    "        \"Functional-Trained\": \"Functional-trained\",\n",
    "        \"Clinical-Trained Meta Predictor\": \"Clinical-trained (meta)\",\n",
    "        \"Clinical-Trained Single Predictor\": \"Clinical-trained (single)\",\n",
    "        \"Population-Free\": \"Population-free\",\n",
    "        \"Population-Tuned\": \"Population-tuned\"\n",
    "    }\n",
    "    df[\"Category\"] = df[\"Category\"].map(category_rename)\n",
    "\n",
    "    color_map = {\n",
    "        \"Functional-trained\": \"#000080\",\n",
    "        \"Clinical-trained (meta)\": \"#b21e35\",\n",
    "        \"Clinical-trained (single)\": \"goldenrod\",\n",
    "        \"Population-free\": \"#98e5a5\",\n",
    "        \"Population-tuned\": \"#23a4a6\",\n",
    "    }\n",
    "    df[\"Color\"] = df[\"Category\"].map(color_map).fillna(\"white\")\n",
    "\n",
    "    df = df.sort_values(score_col, ascending=True).reset_index(drop=True)\n",
    "    n_bars = len(df)\n",
    "\n",
    "    gap_ref = 0.5\n",
    "    gap = max(0.05, gap_ref * gap_frac)\n",
    "    step = bar_height + gap\n",
    "    y_pos = np.arange(n_bars) * step\n",
    "    outer_pad = bar_height * outer_pad_frac\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 5.5,\n",
    "        # 'font.weight': 'medium',\n",
    "        # 'font.family': 'DejaVu Sans'\n",
    "    })\n",
    "\n",
    "    fig_height = max(2.0, 0.28 * n_bars * (step / 1.0))\n",
    "    fig, ax = plt.subplots(figsize=(6, fig_height))\n",
    "\n",
    "    ax.barh(\n",
    "        y=y_pos,\n",
    "        width=df[score_col] if score_col == \"Mean_R2\" else df[score_col] - 0.5,\n",
    "        left=0 if score_col == \"Mean_R2\" else 0.5,\n",
    "        color=df[\"Color\"],\n",
    "        height=bar_height,\n",
    "        edgecolor='black',\n",
    "        linewidth=linewidth\n",
    "    )\n",
    "\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(df[\"Tool\"])\n",
    "    ax.set_ylim(y_pos.min() - outer_pad, y_pos.max() + outer_pad)\n",
    "    ax.set_xlim([0.0 if score_col == \"Mean_R2\" else 0.5, len_x])\n",
    "    ax.set_xlabel(x_label, fontsize=7)\n",
    "    ax.set_title(plot_title, fontsize=9, loc=\"center\", pad=10)\n",
    "\n",
    "    if include_legend:\n",
    "        handles = [Patch(facecolor=color_map[c], label=c, edgecolor='black', linewidth=linewidth) for c in color_map]\n",
    "        ax.legend(\n",
    "            handles=handles,\n",
    "            loc=\"lower right\",\n",
    "            fontsize=8,\n",
    "            title_fontsize=6.5,\n",
    "            frameon=True,\n",
    "            edgecolor=\"gray\"\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(output_figure_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"Figure saved to: {output_figure_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6870cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved to: ../results/figures/benchmarks/clinical_auc_plot.png\n",
      "Figure saved to: ../results/figures/benchmarks/functional_auc_plot.png\n",
      "Figure saved to: ../results/figures/benchmarks/dd_auc_plot.png\n",
      "Figure saved to: ../results/figures/benchmarks/ndd_auc_plot.png\n",
      "Figure saved to: ../results/figures/benchmarks/cancer_auc_plot.png\n",
      "Figure saved to: ../results/figures/benchmarks/mave_gam_auc_plot.png\n"
     ]
    }
   ],
   "source": [
    "generate_benchmark_figure(\"clinical\", include_legend=True, filter_glm=False, len_x = 1.0)\n",
    "generate_benchmark_figure(\"functional\", include_legend=True, filter_glm=False)\n",
    "generate_benchmark_figure(\"dd\", include_legend=True, len_x=0.82)\n",
    "generate_benchmark_figure(\"ndd\", include_legend=True, len_x=0.66)\n",
    "generate_benchmark_figure(\"cancer\", include_legend=True, len_x=0.95)\n",
    "generate_benchmark_figure(\"mave_gam\", include_legend=True, filter_glm=False, len_x=0.27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33968826",
   "metadata": {},
   "source": [
    "#### Generate filtered AUC bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271bdec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import font_manager as fm\n",
    "from pathlib import Path\n",
    "\n",
    "font_path = \"../resources/fonts/Aptos.ttf\"\n",
    "fm.fontManager.addfont(font_path)\n",
    "prop = fm.FontProperties(fname=font_path)\n",
    "mpl.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "def generate_filtered_benchmark_figure(\n",
    "    dataset_name=\"functional\",\n",
    "    bold_keep_tools=False,\n",
    "    remove_clinvep=True,\n",
    "    len_x=None,\n",
    "    linewidth=1,\n",
    "    gap_frac=0.25,\n",
    "    fig_height_per_bar=0.6,\n",
    "    outer_pad_frac = 0.5,\n",
    "    tools_per_category=3,\n",
    "): \n",
    "\n",
    "    if dataset_name.startswith(\"mave_\"):\n",
    "        score_col = \"Mean_R2\"\n",
    "        correlation_type = dataset_name.replace(\"mave_\", \"\")\n",
    "        data_file = f\"../results/benchmarking/mave_{correlation_type}_correlation.txt\"\n",
    "        x_label = \"Mean R²\"\n",
    "        plot_title = \"Mean R² across MAVE studies\"\n",
    "        default_len_x = 0.35\n",
    "    else:\n",
    "        score_col = \"AUC\"\n",
    "        data_file = f\"../results/benchmarking/{dataset_name}.txt\"\n",
    "        x_label = \"AUC\"\n",
    "        default_len_x = 1.0\n",
    "        if dataset_name == \"cancer\":\n",
    "            plot_title = \"Performance on cancer hotspots\"\n",
    "        elif dataset_name in [\"clinical\", \"functional\"]:\n",
    "            plot_title = f\"Performance on {'clinical' if dataset_name == 'clinical' else 'functional'} data\"\n",
    "        else:\n",
    "            plot_title = f\"Performance on {dataset_name.upper()} variants\"\n",
    "\n",
    "    len_x = len_x if len_x is not None else default_len_x\n",
    "    features_file = \"../resources/feature_lists/all_columns.txt\"\n",
    "    output_figure_path = f\"../results/figures/benchmarks/{dataset_name}_auc_filtered_plot.png\"\n",
    "\n",
    "    df = pd.read_csv(data_file, sep=\"\\t\")\n",
    "    features_df = pd.read_csv(features_file, sep=\"\\t\").rename(columns={\"Name\": \"Tool\"})\n",
    "    df = df.merge(features_df[[\"Tool\", \"Category\"]], on=\"Tool\", how=\"left\")\n",
    "\n",
    "    if remove_clinvep:\n",
    "        df = df[~df[\"Tool\"].str.startswith(\"ClinVEP_\")].copy()\n",
    "\n",
    "    always_keep = {\"FuncVEP_CTI\", \"FuncVEP_CTE\", \"FuncVEP_SP\", \"ClinVEP_CTI\", \"ClinVEP_CTE\", \"ClinVEP_SP\"}\n",
    "    remaining_df = df[~df[\"Tool\"].isin(always_keep)]\n",
    "\n",
    "    best_per_category = (\n",
    "        remaining_df.sort_values(score_col, ascending=False)\n",
    "        .groupby(\"Category\")\n",
    "        .head(tools_per_category)\n",
    "    )\n",
    "\n",
    "    selected_df = pd.concat([\n",
    "        df[df[\"Tool\"].isin(always_keep)],\n",
    "        best_per_category\n",
    "    ]).drop_duplicates(subset=\"Tool\")\n",
    "\n",
    "    def clean_tool_name(tool):\n",
    "        tool = re.sub(r\"^glm_\", \"\", tool)\n",
    "        tool = re.sub(r\"_score$\", \"\", tool)\n",
    "        return tool.replace(\"___\", \"-\").replace(\"__\", \"-\").replace(\"_\", \"-\")\n",
    "\n",
    "    selected_df[\"Cleaned_Tool\"] = selected_df[\"Tool\"].apply(clean_tool_name)\n",
    "\n",
    "    category_rename = {\n",
    "        \"Functional-Trained\": \"Functional-trained\",\n",
    "        \"Clinical-Trained Meta Predictor\": \"Clinical-trained (meta)\",\n",
    "        \"Clinical-Trained Single Predictor\": \"Clinical-trained (single)\",\n",
    "        \"Population-Free\": \"Population-free\",\n",
    "        \"Population-Tuned\": \"Population-tuned\"\n",
    "    }\n",
    "    selected_df[\"Category\"] = selected_df[\"Category\"].map(category_rename)\n",
    "\n",
    "    color_map = {\n",
    "        \"Functional-trained\": \"#000080\",\n",
    "        \"Clinical-trained (meta)\": \"#b21e35\",\n",
    "        \"Clinical-trained (single)\": \"goldenrod\",\n",
    "        \"Population-free\": \"#98e5a5\",\n",
    "        \"Population-tuned\": \"#23a4a6\",\n",
    "    }\n",
    "    selected_df[\"Color\"] = selected_df[\"Category\"].map(color_map).fillna(\"white\")\n",
    "\n",
    "    selected_df = selected_df.sort_values(score_col, ascending=True).reset_index(drop=True)\n",
    "\n",
    "    bar_height_ref = 0.5\n",
    "    gap_ref = 1.0 - bar_height_ref\n",
    "    gap = gap_ref * gap_frac\n",
    "    bar_height = bar_height_ref\n",
    "\n",
    "    step = bar_height + gap\n",
    "    n_bars = len(selected_df)\n",
    "    y_pos = np.arange(n_bars) * step\n",
    "\n",
    "    fig_height = max(2.0, fig_height_per_bar * n_bars * (step / 1.0))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, fig_height))\n",
    "\n",
    "    ax.barh(\n",
    "        y=y_pos,\n",
    "        width=selected_df[score_col] - (0.5 if score_col == \"AUC\" else 0.0),\n",
    "        left=(0.5 if score_col == \"AUC\" else 0.0),\n",
    "        color=selected_df[\"Color\"],\n",
    "        height=bar_height,\n",
    "        edgecolor='black',\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(selected_df[\"Cleaned_Tool\"])\n",
    "    ax.set_title(plot_title, fontsize=16, pad=20)\n",
    "    ax.set_xlabel(x_label, fontsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "\n",
    "    pad = bar_height * 0.6\n",
    "    \n",
    "    outer_pad = bar_height * outer_pad_frac\n",
    "\n",
    "    ymin = y_pos.min() - (bar_height / 2) - outer_pad\n",
    "    ymax = y_pos.max() + (bar_height / 2) + outer_pad\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    ax.set_xlim([0.5 if score_col == \"AUC\" else 0.0, len_x])\n",
    "\n",
    "    always_keep_cleaned = {clean_tool_name(t) for t in always_keep}\n",
    "    for tick, label in zip(ax.get_yticklabels(), selected_df[\"Cleaned_Tool\"]):\n",
    "        tick.set_fontweight('bold' if bold_keep_tools and label in always_keep_cleaned else 'normal')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(output_figure_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"Filtered figure saved to: {output_figure_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb48891d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered figure saved to: ../results/figures/benchmarks/clinical_auc_filtered_plot.png\n",
      "Filtered figure saved to: ../results/figures/benchmarks/functional_auc_filtered_plot.png\n",
      "Filtered figure saved to: ../results/figures/benchmarks/dd_auc_filtered_plot.png\n",
      "Filtered figure saved to: ../results/figures/benchmarks/ndd_auc_filtered_plot.png\n",
      "Filtered figure saved to: ../results/figures/benchmarks/cancer_auc_filtered_plot.png\n",
      "Filtered figure saved to: ../results/figures/benchmarks/mave_gam_auc_filtered_plot.png\n"
     ]
    }
   ],
   "source": [
    "generate_filtered_benchmark_figure(\"clinical\")\n",
    "generate_filtered_benchmark_figure(\"functional\")\n",
    "generate_filtered_benchmark_figure(\"dd\", len_x=0.82)\n",
    "generate_filtered_benchmark_figure(\"ndd\", len_x=0.66)\n",
    "generate_filtered_benchmark_figure(\"cancer\", len_x=0.95)\n",
    "generate_filtered_benchmark_figure(\"mave_gam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a384c6",
   "metadata": {},
   "source": [
    "#### Generate MAVE violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "import re\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import font_manager as fm\n",
    "from pathlib import Path\n",
    "\n",
    "font_path = \"../resources/fonts/Aptos.ttf\"\n",
    "fm.fontManager.addfont(font_path)\n",
    "prop = fm.FontProperties(fname=font_path)\n",
    "mpl.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "def plot_mave_violin(\n",
    "    tools_per_category: int = 3,\n",
    "    include_legend: bool = True,\n",
    "    remove_clinvep: bool = True,\n",
    "    remove_varity: bool = False,\n",
    "    *,\n",
    "    x_gap_frac: float = 0.6,\n",
    "    violin_width: float = 0.3,\n",
    "    fig_width_per_tool: float = 0.75,\n",
    "    fig_height: float = 6.0,\n",
    "    left_pad: float = 0.5,\n",
    "    right_pad_frac: float = 0.25,\n",
    "    title_fontsize: int = 16,\n",
    "    ylabel_fontsize: int = 14,\n",
    "    xtick_fontsize: int = 12,\n",
    "    mean_fontsize: int = 9,\n",
    "    ytick_fontsize: int = 14,\n",
    "    y_major_step: float = 0.2,\n",
    "    y_minor_step: float = 0.05,\n",
    "):\n",
    "\n",
    "    gam_df = pd.read_csv(\"../results/benchmarking/mave_gam_correlation.txt\", sep=\"\\t\")\n",
    "    features_df = (\n",
    "        pd.read_csv(\"../resources/feature_lists/all_columns.txt\", sep=\"\\t\")\n",
    "        .rename(columns={\"Name\": \"Tool\"})\n",
    "    )\n",
    "    gam_df = gam_df.merge(features_df[[\"Tool\", \"Category\"]], on=\"Tool\", how=\"left\")\n",
    "\n",
    "    category_rename = {\n",
    "        \"Functional-Trained\": \"Functional-trained\",\n",
    "        \"Clinical-Trained Meta Predictor\": \"Clinical-trained (meta)\",\n",
    "        \"Clinical-Trained Single Predictor\": \"Clinical-trained (single)\",\n",
    "        \"Population-Free\": \"Population-free\",\n",
    "        \"Population-Tuned\": \"Population-tuned\",\n",
    "    }\n",
    "    gam_df[\"Category\"] = gam_df[\"Category\"].map(category_rename)\n",
    "\n",
    "    if remove_clinvep:\n",
    "        gam_df = gam_df[~gam_df[\"Tool\"].str.startswith(\"ClinVEP\")].copy()\n",
    "\n",
    "    if remove_varity:\n",
    "        gam_df = gam_df[~gam_df[\"Tool\"].str.startswith(\"VARITY\")].copy()\n",
    "\n",
    "    color_map = {\n",
    "        \"Functional-trained\": \"#000080\",\n",
    "        \"Clinical-trained (meta)\": \"#b21e35\",\n",
    "        \"Clinical-trained (single)\": \"goldenrod\",\n",
    "        \"Population-free\": \"#98e5a5\",\n",
    "        \"Population-tuned\": \"#23a4a6\",\n",
    "    }\n",
    "\n",
    "    def clean_tool_name(tool: str) -> str:\n",
    "        tool = re.sub(r\"^glm_\", \"\", tool)\n",
    "        tool = re.sub(r\"_score$\", \"\", tool)\n",
    "        return tool.replace(\"___\", \"-\").replace(\"__\", \"-\").replace(\"_\", \"-\")\n",
    "\n",
    "    gam_df = gam_df.sort_values(\"Mean_R2\", ascending=False)\n",
    "    top_per_category = (\n",
    "        gam_df.groupby(\"Category\", group_keys=False)\n",
    "        .head(tools_per_category)\n",
    "        .copy()\n",
    "    )\n",
    "    top_per_category[\"Tool\"] = top_per_category[\"Tool\"].apply(clean_tool_name)\n",
    "\n",
    "    tools = top_per_category[\"Tool\"].tolist()\n",
    "    tool_to_color = {\n",
    "        row[\"Tool\"]: color_map.get(row[\"Category\"], \"gray\")\n",
    "        for _, row in top_per_category.iterrows()\n",
    "    }\n",
    "    gam_long = (\n",
    "        top_per_category.melt(\n",
    "            id_vars=[\"Tool\", \"Mean_R2\", \"Category\"],\n",
    "            var_name=\"Source\",\n",
    "            value_name=\"R2\",\n",
    "        )\n",
    "        .dropna(subset=[\"R2\"])\n",
    "    )\n",
    "\n",
    "    sns.set(style=\"white\", context=\"talk\", font_scale=1.2)\n",
    "\n",
    "    y_max = 0.8\n",
    "    label_y = y_max + 0.01\n",
    "\n",
    "    x_step = x_gap_frac\n",
    "    x_positions = np.arange(len(tools)) * x_step\n",
    "\n",
    "    max_safe_width = x_step * 0.95\n",
    "    if violin_width > max_safe_width:\n",
    "        print(\n",
    "            f\"[plot_mave_violin] violin_width={violin_width:.2f} too large for \"\n",
    "            f\"x_step={x_step:.2f}; clamping to {max_safe_width:.2f}.\"\n",
    "        )\n",
    "        violin_width = max_safe_width\n",
    "    width = violin_width\n",
    "\n",
    "    fig_w = max(8.0, len(tools) * fig_width_per_tool * x_gap_frac)\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_height))\n",
    "\n",
    "    for i, tool in enumerate(tools):\n",
    "        data = gam_long[gam_long[\"Tool\"] == tool][\"R2\"].dropna().values\n",
    "        if len(data) < 2:\n",
    "            continue\n",
    "\n",
    "        kde = gaussian_kde(data, bw_method=0.2)\n",
    "        y_vals = np.linspace(0, y_max, 200)\n",
    "        densities = kde(y_vals)\n",
    "        densities = densities / densities.max() * width\n",
    "\n",
    "        ax.fill_betweenx(\n",
    "            y_vals,\n",
    "            x_positions[i],\n",
    "            x_positions[i] + densities,\n",
    "            facecolor=tool_to_color[tool],\n",
    "            alpha=0.7,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            [x_positions[i], x_positions[i]],\n",
    "            [0, y_max],\n",
    "            color=\"k\",\n",
    "            lw=1,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        mean_val = np.mean(data)\n",
    "        ax.plot(\n",
    "            x_positions[i],\n",
    "            mean_val,\n",
    "            \"o\",\n",
    "            color=\"black\",\n",
    "            markersize=6,\n",
    "            zorder=10,\n",
    "        )\n",
    "        ax.text(\n",
    "            x_positions[i],\n",
    "            label_y,\n",
    "            f\"{mean_val:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=mean_fontsize,\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(tools, rotation=45, ha=\"right\", fontsize=xtick_fontsize)\n",
    "    ax.set_ylabel(\"GAM $R^2$\", fontsize=ylabel_fontsize)\n",
    "    ax.set_ylim(0, label_y + 0.03)\n",
    "    ax.set_title(\"MAVE correlation performance\", pad=20, fontsize=title_fontsize)\n",
    "\n",
    "    right_pad = x_step * right_pad_frac\n",
    "    ax.set_xlim(-left_pad, x_positions[-1] + width + right_pad)\n",
    "\n",
    "    sns.despine(ax=ax, top=True, right=True, left=False, bottom=False, trim=False)\n",
    "\n",
    "    ax.set_yticks(np.arange(0, 0.8001 + 1e-9, y_major_step))\n",
    "    ax.set_yticks(np.arange(0, 0.8001 + 1e-9, y_minor_step), minor=True)\n",
    "\n",
    "    ax.tick_params(axis=\"y\", which=\"major\", direction=\"inout\", length=5, width=1.2, labelsize=ytick_fontsize)\n",
    "    ax.tick_params(axis=\"y\", which=\"minor\", direction=\"inout\", length=3, width=1)\n",
    "\n",
    "    ax.grid(axis=\"y\", which=\"major\", linestyle=\"-\", linewidth=0.6, color=\"0.85\")\n",
    "    ax.grid(axis=\"y\", which=\"minor\", linestyle=\"-\", linewidth=0.4, color=\"0.92\")\n",
    "\n",
    "    if include_legend:\n",
    "        handles = [\n",
    "            plt.Line2D(\n",
    "                [0], [0],\n",
    "                marker=\"o\",\n",
    "                color=\"w\",\n",
    "                label=cat,\n",
    "                markerfacecolor=color,\n",
    "                markeredgecolor=\"black\",\n",
    "                markersize=10,\n",
    "            )\n",
    "            for cat, color in color_map.items()\n",
    "        ]\n",
    "        ax.legend(\n",
    "            handles=handles,\n",
    "            loc=\"upper left\",\n",
    "            bbox_to_anchor=(0.98, 0.85),\n",
    "            title=\"Category\",\n",
    "            fontsize=12,\n",
    "            title_fontsize=14,\n",
    "            frameon=True,\n",
    "            edgecolor=\"gray\",\n",
    "        )\n",
    "\n",
    "    out_base = f\"../results/figures/benchmarks/mave_gam_violin_plot\"\n",
    "    out_base.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.tight_layout(rect=[0.01, 0, 0.93, 1])\n",
    "    fig.savefig(f\"{out_base}.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved: {out_base}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b3445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../results/figures/benchmarks/mave_gam_violin_plot.png\n"
     ]
    }
   ],
   "source": [
    "plot_mave_violin(tools_per_category=3, include_legend=False, remove_clinvep=True, remove_varity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e854b72",
   "metadata": {},
   "source": [
    "#### Plot ACMG classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaaf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import font_manager as fm\n",
    "from pathlib import Path\n",
    "\n",
    "font_path = \"../resources/fonts/Aptos.ttf\"\n",
    "fm.fontManager.addfont(font_path)\n",
    "prop = fm.FontProperties(fname=font_path)\n",
    "mpl.rcParams['font.family'] = prop.get_name()\n",
    "\n",
    "df = pd.read_csv(\"../results/benchmarking/acmg_performance.txt\", sep=\"\\t\")\n",
    "df[\"Conclusiveness\"] = 1 - df[\"VUS_M ratio\"]\n",
    "\n",
    "metrics = [\"Sensitivity\", \"Specificity\", \"Concordance\", \"Conclusiveness\"]\n",
    "\n",
    "color_map = {\n",
    "    \"AlphaMissense\": \"#23a4a6\",\n",
    "    \"ESM1b\": \"#98e5a5\",\n",
    "    \"FuncVEP-CTI\": \"#001c71\",\n",
    "    \"FuncVEP-CTE\": \"#2743b5\",\n",
    "    \"FuncVEP-SP\": \"#8fa3ff\",\n",
    "}\n",
    "\n",
    "bar_width = 0.15\n",
    "x = np.arange(len(metrics))\n",
    "offsets = np.linspace(-2, 2, len(df)) * bar_width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "for i, tool in enumerate(df[\"Tool\"]):\n",
    "    y_vals = df.loc[i, metrics].values.astype(float)\n",
    "    x_positions = x + offsets[i]\n",
    "\n",
    "    ax.bar(\n",
    "        x_positions,\n",
    "        y_vals,\n",
    "        width=bar_width,\n",
    "        color=color_map[tool],\n",
    "        edgecolor=\"black\",\n",
    "        label=tool,\n",
    "        linewidth=0.4,\n",
    "    )\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, fontsize=9)\n",
    "ax.set_ylabel(\"Performance\", fontsize=10)\n",
    "ax.set_ylim([0.70, 1.0])\n",
    "ax.set_title(\"ACMG Classification Performance\", fontsize=11)\n",
    "ax.tick_params(axis=\"y\", labelsize=8)\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(\n",
    "        facecolor=color_map[t],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.6,\n",
    "        label=t\n",
    "    )\n",
    "    for t in df[\"Tool\"]\n",
    "]\n",
    "\n",
    "ax.legend(\n",
    "    handles=legend_elements,\n",
    "    fontsize=8.5,\n",
    "    frameon=False,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, -0.22),\n",
    "    ncol=5\n",
    ")\n",
    "\n",
    "Path(\"../results/figures/benchmarks\").mkdir(parents=True, exist_ok=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/figures/benchmarks/acmg_benchmark.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0d20b",
   "metadata": {},
   "source": [
    "#### Generate benchmark tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_benchmark_table(dataset_name=\"clinical\", filter_glm=False):\n",
    "\n",
    "    other_dataset = \"functional\" if dataset_name == \"clinical\" else \"clinical\"\n",
    "    benchmark_file = f\"../results/benchmarking/{dataset_name}.txt\"\n",
    "    other_benchmark_file = f\"../results/benchmarking/{other_dataset}.txt\"\n",
    "    features_file = \"../resources/feature_lists/all_columns.txt\"\n",
    "    output_file = f\"../results/tables/benchmarks/{dataset_name}_benchmark.txt\"\n",
    "\n",
    "    benchmark_df = pd.read_csv(benchmark_file, sep=\"\\t\")\n",
    "    other_df = pd.read_csv(other_benchmark_file, sep=\"\\t\")\n",
    "    features_df = pd.read_csv(features_file, sep=\"\\t\").rename(columns={\"Name\": \"Tool\"})\n",
    "\n",
    "    included_types = [\n",
    "        \"Variant Effect Predictor\",\n",
    "    ]\n",
    "    included_tools = features_df[features_df[\"Category\"].isin(included_types)][\"Tool\"].unique()\n",
    "    benchmark_df = benchmark_df[~benchmark_df[\"Tool\"].isin(included_tools)].copy()\n",
    "\n",
    "    if filter_glm:\n",
    "        def get_top_glm_tool(df):\n",
    "            glm_subset = df[df[\"Tool\"].str.startswith(\"glm_\")]\n",
    "            return glm_subset.loc[glm_subset[\"AUC\"].idxmax(), \"Tool\"] if not glm_subset.empty else None\n",
    "\n",
    "        top_glm_current = get_top_glm_tool(benchmark_df)\n",
    "        top_glm_other = get_top_glm_tool(other_df)\n",
    "        glm_to_keep = set(filter(None, [top_glm_current, top_glm_other]))\n",
    "\n",
    "        benchmark_df = benchmark_df[\n",
    "            ~benchmark_df[\"Tool\"].str.startswith(\"glm_\") | benchmark_df[\"Tool\"].isin(glm_to_keep)\n",
    "        ].copy()\n",
    "\n",
    "    benchmark_df = benchmark_df.merge(features_df[[\"Tool\", \"Category\"]], on=\"Tool\", how=\"left\")\n",
    "\n",
    "    def clean_tool_name(tool):\n",
    "        tool = re.sub(r\"^glm_\", \"\", tool)\n",
    "        tool = re.sub(r\"_score$\", \"\", tool)\n",
    "        tool = tool.replace(\"___\", \"-\").replace(\"__\", \"-\").replace(\"_\", \"-\")\n",
    "        return tool\n",
    "\n",
    "    benchmark_df[\"Tool\"] = benchmark_df[\"Tool\"].apply(clean_tool_name)\n",
    "\n",
    "    final_df = benchmark_df[[\"Tool\", \"AUC\", \"Accuracy\", \"Category\"]].copy()\n",
    "    final_df.sort_values(\"AUC\", ascending=False, inplace=True)\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    final_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Benchmark table saved to: {output_file}\")\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157df7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark table saved to: ../results/tables/benchmarks/clinical_benchmark.txt\n",
      "Benchmark table saved to: ../results/tables/benchmarks/functional_benchmark.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_benchmark_table(\"clinical\", filter_glm=False)\n",
    "generate_benchmark_table(\"functional\", filter_glm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633268ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_average_auc_by_category(dataset_name=\"clinical\", filter_glm=False):\n",
    "\n",
    "    benchmark_df = generate_benchmark_table(dataset_name, filter_glm)\n",
    "    category_auc_df = benchmark_df.groupby(\"Category\", as_index=False)[\"AUC\"].mean()\n",
    "    category_auc_df.sort_values(\"AUC\", ascending=False, inplace=True)\n",
    "    output_file = f\"../results/tables/benchmarks/{dataset_name}_average_auc_by_category.txt\"\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    category_auc_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Average AUC by category table saved to: {output_file}\")\n",
    "    return category_auc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ae1906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark table saved to: ../results/tables/benchmarks/clinical_benchmark.txt\n",
      "Average AUC by category table saved to: ../results/tables/benchmarks/clinical_average_auc_by_category.txt\n",
      "Benchmark table saved to: ../results/tables/benchmarks/functional_benchmark.txt\n",
      "Average AUC by category table saved to: ../results/tables/benchmarks/functional_average_auc_by_category.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Functional-Trained</td>\n",
       "      <td>0.960354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinical-Trained Single Predictor</td>\n",
       "      <td>0.808367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Population-Free</td>\n",
       "      <td>0.800284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Population-Tuned</td>\n",
       "      <td>0.759257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clinical-Trained Meta Predictor</td>\n",
       "      <td>0.756777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Category       AUC\n",
       "2                 Functional-Trained  0.960354\n",
       "1  Clinical-Trained Single Predictor  0.808367\n",
       "3                    Population-Free  0.800284\n",
       "4                   Population-Tuned  0.759257\n",
       "0    Clinical-Trained Meta Predictor  0.756777"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_average_auc_by_category(\"clinical\", filter_glm=False)\n",
    "save_average_auc_by_category(\"functional\", filter_glm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark table saved to: ../results/tables/benchmarks/functional_benchmark.txt\n",
      "Average AUC by category table saved to: ../results/tables/benchmarks/functional_average_auc_by_category.txt\n",
      "Benchmark table saved to: ../results/tables/benchmarks/clinical_benchmark.txt\n",
      "Average AUC by category table saved to: ../results/tables/benchmarks/clinical_average_auc_by_category.txt\n",
      "Merged average AUC table saved to: ../results/tables/benchmarks/merged_average_auc_by_category.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Average AUC on functional data</th>\n",
       "      <th>Average AUC on clinical data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Functional-Trained</td>\n",
       "      <td>0.960354</td>\n",
       "      <td>0.970876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinical-Trained Single Predictor</td>\n",
       "      <td>0.808367</td>\n",
       "      <td>0.923161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Population-Free</td>\n",
       "      <td>0.800284</td>\n",
       "      <td>0.905156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Population-Tuned</td>\n",
       "      <td>0.759257</td>\n",
       "      <td>0.879514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clinical-Trained Meta Predictor</td>\n",
       "      <td>0.756777</td>\n",
       "      <td>0.910209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Category  Average AUC on functional data  \\\n",
       "2                 Functional-Trained                        0.960354   \n",
       "1  Clinical-Trained Single Predictor                        0.808367   \n",
       "3                    Population-Free                        0.800284   \n",
       "4                   Population-Tuned                        0.759257   \n",
       "0    Clinical-Trained Meta Predictor                        0.756777   \n",
       "\n",
       "   Average AUC on clinical data  \n",
       "2                      0.970876  \n",
       "1                      0.923161  \n",
       "3                      0.905156  \n",
       "4                      0.879514  \n",
       "0                      0.910209  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_and_merge_average_auc_by_category(filter_glm=False):\n",
    "\n",
    "    functional_df = save_average_auc_by_category(dataset_name=\"functional\", filter_glm=filter_glm)\n",
    "    clinical_df = save_average_auc_by_category(dataset_name=\"clinical\", filter_glm=filter_glm)\n",
    "\n",
    "    functional_df.rename(columns={\"AUC\": \"Average AUC on functional data\"}, inplace=True)\n",
    "    clinical_df.rename(columns={\"AUC\": \"Average AUC on clinical data\"}, inplace=True)\n",
    "\n",
    "    merged_df = pd.merge(functional_df, clinical_df, on=\"Category\", how=\"outer\")\n",
    "    merged_df.sort_values(by=\"Average AUC on functional data\", ascending=False, inplace=True, na_position='last')\n",
    "\n",
    "    output_file = \"../results/tables/benchmarks/merged_average_auc_by_category.txt\"\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    merged_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Merged average AUC table saved to: {output_file}\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "save_and_merge_average_auc_by_category()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2675862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
